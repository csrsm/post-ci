---
category: csrsm
title: 大数据基础
header-title: true
header-image:
  - /img/background.jpg
  - /img/001.png
  - /img/0002.png
  - /img/0003.png
  - /img/0004.png
  - /img/0005.png
  - /img/0006.png
  - /img/0007.png
  - /img/0008.png
  - /img/0009.png
  - /img/0010.png
  - /img/0011.png
  - /img/0012.png
  - /img/0013.png
  - /img/0014.png
  - /img/0015.png
  - /img/0016.png
tags:
  - 大数据基础
date: 2022-5-2
---

## 1.数据仓库分层

  <img :src="$withBase('/img/bigdata/数仓分层.png')" alt="单架构">

--------------------------------------------范式建模begin----------------------------------------------------------

1. **ODS层（operational data source）：**操作数据层，直接存放业务系统抽取过来的数据，将不同业务系统中的数据汇聚在一起

2. **DW层（data warehouse）：**数据仓库，按照主题建立各种数据模型

- **DWD层（data warehouse detail）：**数据明细层，保证数据质量，在ods的基础上对数据进行加工处理，提供更干净的数据
  --------------------------------------------范式建模bend----------------------------------------------------------  

--------------------------------------------维度建模begin----------------------------------------------------------

- **DWM层（data warehouse middle）：**数据中间层，对通用的维度进行轻度聚合，计算相应的统计指标，方便复用
- **DWS层（data warehouse service）：**数据服务层，按照主题业务组织主题宽表，用于OLAP分析

3. **DM层（data market）：**数据集市层，基于DW层的基础数据，整合汇总成分析某一个主题域的报表数据
   --------------------------------------------维度建模end----------------------------------------------------------

**分层的目的：**

1. 清晰的数据结构
2. 减少重复开发
3. 统一数据出口
4. 简化问题

## 2.数据库-OLTP与数据仓库的区别-OLAP

| 功能       | 数据库                       | 数据差仓库                     |
| ---------- | ---------------------------- | ------------------------------ |
| 数据范围   | 当前状态数据                 | 存储完整、反映历史变化数据     |
| 数据变化   | 支持频繁的增删改查           | 可增加、查询、无更新、删除操作 |
| 应用场景   | 面向业务交易流程             | 面向分析、侧重决策分析         |
| 处理数据量 | 频繁、小批次、高并发、低延迟 | 非频繁、大批量、高吞吐、有延迟 |
| 设计理论   | 遵循数据库三范式、避免冗余   | 违范式、适当冗余               |
| 建模方式   | ER实体关系建模（范式建模）   | 范式建模+维度建模              |

## 3.大数据技术栈

### 1.架构底层核心技术：

1. JVM
2. 多线程、高并发
3. IO

### 2.数据采集体系技术

1. sqoop
2. DataX
3. Kettle
4. Flume
5. Maxwell
6. Canal
7. Nifi

### 3.中间件技术栈

1. zookeeper 分布式协调服务
2. redis 分布式缓存
3. kafka 分布式消息系统
4. Pulsar 分布式消息系统
5. ELK stack 数据分析

### 4.分布式存储

1. HDFS 分布式文件系统
2. HBase 分布式数据库
3. Hive 分布式数仓
4. Hudi、Delta lack、Iceberg 数据湖技术

### 5.数据处理技术

1. MapReduce 分布式计算框架
2. Spark 分布式计算框架
3. Flink 分布式计算框架

### 6.OLAP生态系统

（1）OLAP-Kylin
（2）OLAP-Presto
（3）OLAP-Druid
（4）OLAP-Impala
（5）OLAP-ClickHouse
（6）OLAP-Phoenix、Kudu、Doirs等等

### 7.稳健架构设计

1. 离线数仓架构方法论
2. 实时数仓架构方法论
3. 数据治理-数据质量管理
4. 数据治理-元数据管理
5. 数据治理-数据安全管理
6. Kerberos
7. 数据中台构建方法论
8. 可视化

### 8.集群调度体系

1. 分布式资源调度 Yarn
2. 任务流调度 oozie
3. 任务流调度 Azkaban
4. 任务流调度 Airflow
5. 集群管理平台 cloudera manager
6. 集群管理平台 Ambari

### 9.数据挖掘体系相关

1. python
2. 多元线性回归
3. 贝叶斯算法
4. KNN算法
5. KMeans算法
6. KMeans++
7. TF-IDF
8. 逻辑回归
9. 决策树
10. 随机森林等等



## 4.大数据架构演变

### 1.Lambda架构（离线处理+实时链路）

  <img :src="$withBase('/img/bigdata/传统离线架构.jpg')" alt="传统离线架构">

  <img :src="$withBase('/img/bigdata/Lambda架构-传统实时开发.jpg')" alt="Lambda架构-传统实时开发">

#### 1.离线处理

- 数据采集：Sqoop、DataX
- 基于Hive的离线数仓：ODS、DWD、DWM、DWS、DM
- 离线结果查询：Impala、Mysql （T+1）

#### 2.实时处理

- 数据采集：Flume、Canal、Maxwell
- 实时计算链路：Kafka、SparkStream、Flink
- 实时结果存储：HBase、Redis、Mysql

#### 3. 缺点

- 实时链路烟囱式开发，数据不能复用

### 2.Lambda架构（离线数仓+实时数仓）

  <img :src="$withBase('/img/bigdata/Lambda架构-实时数仓.jpg')" alt="Lambda架构-实时数仓">

#### 1.离线处理

- 数据采集：Sqoop、DataX
- 基于Hive的离线数仓：ODS、DWD、DWM、DWS、DM
- 离线结果查询：Impala、Mysql （T+1）

#### 2.实时处理

- 数据采集：Flume、Canal、Maxwell
- 实时数据仓库：ODS+Kafka、DWD+Kafka、DWM+Kafka、DWS+Kafka、SparkStream、DM+HBase或Redis或Mysql或ES

#### 3.缺点

- 同样需求需要开发两套一样的代码
- 集群资源使用增多
- 计算结果不一致，以离线结果为主
- 批量计算T+1可能计算不完，资源不变，数据量越来越大
- 服务器存储大

### 3.Kappa架构

<img :src="$withBase('/img/bigdata/Kappa架构.jpg')" alt="Kappa架构">

#### 1.实时数仓

- 数据采集：Flume、Canal、Maxwell
- 实时数据仓库：ODS+Kafka、DWD+Kafka、DWM+Kafka、DWS+Kafka、SparkStream、DM+HBase或Redis或Mysql或ES

#### 2.缺点

- Kafka无法支撑海量数据存储
- Kafka不支持SQL查询，无法支持高效的OLAP
- 无法复用数据血缘管理体系（错误溯源）
- 为保证数仓计算数据的正确性，需要对数据进行更新，Kafka不支持update、upsert

### 4.架构选择

#### 1.公司刚上大数据或者公司业务没有实时场景

选择传统离线大数据架构

#### 2.公司离线业务多，实时业务少

离线数仓+实时链路的Lambda架构

#### 3.公司离线业务和实时业务都比较多

离线数仓+实时数仓的Lambda架构

#### 4.公司实时业务多，离线业务相对少

Kappa纯实时数仓架构

> 绝大多数公司采用Lambda架构
>
> 互联网公司实时业务多混合架构（实时业务采用Kappa，关键核心业务使用离线全量计算方式Lambda）

#### 5.一些互联网公司的架构示例

<img :src="$withBase('/img/bigdata/滴滴架构.jpg')" alt="滴滴架构">

<img :src="$withBase('/img/bigdata/顺丰实时数仓架构.jpg')" alt="顺丰实时数仓架构">

<img :src="$withBase('/img/bigdata/网易业务链.jpg')" alt="网易业务链">

<img :src="$withBase('/img/bigdata/腾讯原有架构.jpg')" alt="腾讯原有架构">

<img :src="$withBase('/img/bigdata/腾讯改造后架构.jpg')" alt="腾讯改造后架构">

### 5.湖仓一体实时数据架构

<img :src="$withBase('/img/bigdata/湖仓一体实时数仓.jpg')" alt="湖仓一体实时数仓">

> 批流一体：既可以进行批处理，又可以进行流处理
>
> 1.架构角度
>
> 2.计算框架处理角度
>
> 3.SQL支持角度
>
> 4.存储层面

#### 1.解决了以上架构的痛点：

- 存储统一
- Kafka存储量小的问题
- 任意分层都可以进行OLAP数据分析
- 复用同一套相同的血缘关系
- 实时数据更新

#### 2.缺点

- 技术较新，存在较多的bug
- 数据湖技术数据存储速度没有Kafka快

## 5.Hadoop

海量数据的分布式存储和计算平台，包含：

- Hadoop Common：基础组件，包括数据IO，数据类型，序列化，安全等等
- Hadoop Distributed File System （HDFS）：分布式存储数据的文件管理系统，是在Linux的文件系统之上套了一层管理文件的软件技术
- Hadoop Yarn：作业调度和资源管理平台
- Hadoop MapReduce：分布式计算模型，适合批处理的计算任务

> 批处理与流处理的区别：
>
> 批处理：有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。
>
> 流处理：无界、实时，无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。
>
> spark与flink的区别：
>
> spark：一切都是由批次组成的，离线数据是一个大批次，而实时数据是有一个一个无限的小批次组成的。
>
> flink：一切都是由流组成的，离线数据是一个有界的流，而实时数据是一个没有界限的流，这就是所谓的有界流和无界流。



### 1.HDFS

#### 1.存储模型

- 文件线性按字节切割成块（block），具有offset、id
- 文件与文件的block大小可以不一样
- 一个文件除最后一个block，其他block大小一致
- block的大小依据硬件的I/O特性调整
- block被分散存放在集群的节点中，具有location（块在哪）
- block具有副本（replication），没有主从的概念，副本不能出现在同一个节点上
- 副本是满足可靠性和性能的关键
- 文件上传可以指定block大小和副本数，上传后只能修改副本数
- 一次写入多次读取，不支持修改
- 支持追加数据

#### 2.架构设计

- HDFS时一个主从架构
- 有一个NameNode和一些DataNode组成
- 面向文件包含：文件数据（data）和文件元数据（medadata）
- NameNode负责存储和管理文件元数据，并维护了一个层次型的文件目录树
- DataNode负责存储文件数据（block），并提供block的读写
- DataNode与NameNode维持心跳，并汇报自己持有的block信息
- Client和NameNode交互文件元数据和DataNode交互文件block数据

#### 3.角色功能

角色即JVM进程，NameNode、DataNode都是一个JVM进程

**NameNode：**

- 完全基于内存存储文件元数据、目录结构、文件block的映射
- 需要持久化方案保证数据可靠性
- 提供副本放置策略

**DataNode：**

- 基于本地磁盘存储block
- 并保存block的校验和保证数据block的可靠性
- 与NameNode保持心跳、汇报block列表状态 

#### 4.元数据持久化

- 任何对文件系统元数据产生修改的操作，NameNode都会使用一种成为EditLog的事务日志记录下来（记录增删改操作）

- 使用FsImage存储内存所有的元数据状态（快照/镜像，间隔发生，内存全量数据基于某个时间点做的向磁盘的溢写，瓶颈在I/O）
- 使用本地磁盘保存EditLog和FsImage
- EditLog具有完整性，数据丢失少，但恢复速度慢，并有体积膨胀风险
- FsImage具有恢复速度快，体积与内存数据相当，但不能实时保存，数据丢失多
- NameNode使用了FsImage+EditLog整合的方案：
  - 滚定将增量的EditLog更新到FsImage，以保证更新近时点的FsImage和更小的EditLog体积
    - 最近时点的FI + 增量的EL
    - 10点  ===>  9点的FI  + 9点到10点的EL，下次开机时，加载FI，加载EL，内存就得到了上次关机前全量的数据

#### 5.安全模式

- HDFS搭建时会格式化，格式化操作会产生一个空的FsImage
- 当NameNode启动时，它从硬盘中读取EditLog和FsImage
- 将所有EditLog中的事务作用在内存中的FsImage上
- 并将这个新版本的FsImage从内存中保存到本地磁盘上
- 然后删除旧的EditLog，因为这个旧的EditLog的事务都已经作用在FsImage上了
- NameNode启动后会进入一个称为安全模式的特殊状态
- 处于安全模式的NameNode是不会进行数据块的复制的
- NameNode从所有的DataNode接收心跳信号和状态报告
- 每当NameNode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为时副本安全的（safely replicated）
- 在一定百分比（可配）的数据块被NameNode检测确认是安全之后（加上一个额外的30秒等待时间），NameNode将退出安全模式状态
- 接下来它会确定还有那些数据块的副本没有到达指定数目，并将这些数据块复制到其他DataNode上

> ​     NameNode存元数据，文件属性、每个块在哪个DataNode上。在持久化的时候，文件属性会持久化，但是文件的每一个块不会持久化，恢复的时候，NameNode会丢失块的位置信息，需要等DataNode与NameNode建立心跳，汇报块的信息。

#### 6.HDFS中的SNN（SecondaryNameNode）

  <img :src="$withBase('/img/bigdata/SNN.jpg')" alt="SNN">

- 在非Ha模式下（非高可用，即只有一个NameNode，企业一般不会用这种模式，NameNode一旦挂掉，整个服务就会下线），SNN一般是独立的节点，周期完成对NameNode的EditLog向FsImage合并，减少EditLog大小，减少SNN启动时间
- 根据配置文件设置的时间间隔fs.checkpoint.period 默认是3600秒
- 根据配置文件设置的edits log大小 fs.checkpoint.size 规定edits文件的最大值默认是64MB

#### 7.Block的副本放置策略

- 第一个副本：放置在上传文件的DataNode；如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。
- 第二个副本：放置在与第一个副本不同的机架的节点上。
- 第三个副本：与第二个副本相同机架的节点（在同一个交换机下，降低成本）。
- 更多副本：随机节点。（机架即机房放置服务器的架子，同一个机架上的服务器有自己的交换机和ups电源）

#### 8.HDFS读写流程

<img :src="$withBase('/img/bigdata/HDFS写流程.jpg')" alt="HDFS写流程">

##### 写流程

- Client和NameNode连接创建文件元数据
- NameNode判定元数据是否有效
- NameNode处发副本放置策略，返回一个有序的DataNode列表
- Client和DataNode建立Pipeline连接
- Client将块切分成packet（64kb），并使用chunk（512b）+chunksum（4b）填充
- Client将packet放入发送队列dataqueue中，并向第一个DataNode发送
- 第一个DataNode收到packet后本地保存并发送给第二个DataNode
- 第二个DataNode收到packet后本地保存并发送给第三个DataNode 
- 这一个过程中，上游节点同时发送下一个packet（生活中类比工厂的流水线，结论：流式其实也是变种的并行计算）
- HDFS使用这种传输方式，副本数对于Client是透明的
- 当block传输完成，DataNode们各自向NameNode汇报，同时Client继续传输下一个block
- 所以，Client的传输和block的汇报也是并行的



<img :src="$withBase('/img/bigdata/HDFS读流程.jpg')" alt="HDFS读流程">

##### 读流程

- 为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本
- 如果在读取程序的同一个机架上有一个副本，那么就读取该副本
- 如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读取本地的数据中心的副本
- 语义：下载一个文件：
  - Client与NameNode交互文件元数据获取fileBlockLocation
  - NameNode会按距离策略排序返回
  - Client尝试下载block并校验数据完整性
- 语义：下载一个文件其实是获取文件的所有block元数据，那么子集获取某些block应该成立
  - HDFS支持Client给出文件的offset自定义连接哪些block的DataNode，自定义获取数据
  - 这个是支持计算层的分治，并行计算的核心

## 6.数据湖

数据湖就是集中式的数据存储库，可以存储各自数据格式数据，例如：非机构化数据、结构化数据、文本、视频......（待续）

